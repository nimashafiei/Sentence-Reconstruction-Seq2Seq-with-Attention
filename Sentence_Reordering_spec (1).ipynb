{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "33aa2b316db44827b700c41921ea0136": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_230441a10bd747dc8215f7158648f040",
              "IPY_MODEL_de9d6a8ca7d14ac286463a948e76428d",
              "IPY_MODEL_0d37fe12eeb54cd1bb812f39aa415ae0"
            ],
            "layout": "IPY_MODEL_3fccafa4e3f046fd80f274ceb0e1adc1"
          }
        },
        "230441a10bd747dc8215f7158648f040": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6fc7d5c52b144fd18b224078e9926ec3",
            "placeholder": "​",
            "style": "IPY_MODEL_0e55044647fa48d29c47f95b4c40d573",
            "value": "100%"
          }
        },
        "de9d6a8ca7d14ac286463a948e76428d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fad8b76cb7914d49af045cae6ef5cf06",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4ceb8870bf294bfc80309bfbbf9a03a0",
            "value": 1
          }
        },
        "0d37fe12eeb54cd1bb812f39aa415ae0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44abf1c38b4242af9da71b082fd4e69e",
            "placeholder": "​",
            "style": "IPY_MODEL_968d06b4237444929c0f3c55ba3e61eb",
            "value": " 1/1 [00:00&lt;00:00,  3.78it/s]"
          }
        },
        "3fccafa4e3f046fd80f274ceb0e1adc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fc7d5c52b144fd18b224078e9926ec3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e55044647fa48d29c47f95b4c40d573": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fad8b76cb7914d49af045cae6ef5cf06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ceb8870bf294bfc80309bfbbf9a03a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "44abf1c38b4242af9da71b082fd4e69e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "968d06b4237444929c0f3c55ba3e61eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sentence Reconstruction"
      ],
      "metadata": {
        "id": "ElNaMbLnRdHR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of this project is to take in input a sequence of words corresponding to a random permutation of a given english sentence, and reconstruct the original sentence. \n",
        "\n",
        "The otuput can be either produced in a single shot, or through an iterative (autoregressive) loop generating a single token at a time.\n",
        "\n",
        "CONSTRAINTS:\n",
        "* No pretrained model can be used.\n",
        "* The neural network models should have less the 20M parameters.\n"
      ],
      "metadata": {
        "id": "oXr4iGUGRms8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "\n",
        "The dataset is composed by a snapshot of wikipedia. We restricted the vocabolary to the 10K most frequent words, and only took sentences making use of this vocabulary. In addition, we restricted to sequences with a length between 3 and 30 words.\n",
        "\n",
        "(Ignore the error, if any) "
      ],
      "metadata": {
        "id": "iQ8k-L-WUK7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip3 install apache-beam"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xmXLLfaUKA6",
        "outputId": "367214e7-bc98-4154-fd2e-16a48f758685"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.15.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets)\n",
            "  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: dill\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.1.1\n",
            "    Uninstalling dill-0.3.1.1:\n",
            "      Successfully uninstalled dill-0.3.1.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "apache-beam 2.48.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dill-0.3.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: apache-beam in /usr/local/lib/python3.10/dist-packages (2.48.0)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.10/dist-packages (from apache-beam) (1.7)\n",
            "Requirement already satisfied: orjson<4.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam) (3.9.1)\n",
            "Collecting dill<0.3.2,>=0.3.1.1 (from apache-beam)\n",
            "  Using cached dill-0.3.1.1-py3-none-any.whl\n",
            "Requirement already satisfied: cloudpickle~=2.2.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam) (2.2.1)\n",
            "Requirement already satisfied: fastavro<2,>=0.23.6 in /usr/local/lib/python3.10/dist-packages (from apache-beam) (1.7.4)\n",
            "Requirement already satisfied: fasteners<1.0,>=0.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam) (0.18)\n",
            "Requirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam) (1.54.0)\n",
            "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam) (2.7.0)\n",
            "Requirement already satisfied: httplib2<0.23.0,>=0.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam) (0.21.0)\n",
            "Requirement already satisfied: numpy<1.25.0,>=1.14.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam) (1.22.4)\n",
            "Requirement already satisfied: objsize<0.7.0,>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam) (0.6.1)\n",
            "Requirement already satisfied: pymongo<5.0.0,>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam) (4.3.3)\n",
            "Requirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam) (1.22.2)\n",
            "Requirement already satisfied: protobuf<4.24.0,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam) (3.20.3)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam) (2022.7.1)\n",
            "Requirement already satisfied: regex>=2020.6.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam) (2022.10.31)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam) (4.5.0)\n",
            "Requirement already satisfied: zstandard<1,>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam) (0.21.0)\n",
            "Requirement already satisfied: pyarrow<12.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam) (9.0.0)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.10/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam) (0.6.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam) (1.16.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<0.23.0,>=0.8->apache-beam) (3.0.9)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from pymongo<5.0.0,>=3.8.0->apache-beam) (2.3.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam) (3.4)\n",
            "Installing collected packages: dill\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.6\n",
            "    Uninstalling dill-0.3.6:\n",
            "      Successfully uninstalled dill-0.3.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "multiprocess 0.70.14 requires dill>=0.3.6, but you have dill 0.3.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dill-0.3.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from random import Random\n",
        "\n",
        "# Instantiate the Random instance with random seed = 42 to ensure reproducibility\n",
        "randomizer = Random(42)"
      ],
      "metadata": {
        "id": "INZIMG8itLHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRVmQCKdRb54"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical, pad_sequences\n",
        "import numpy as np \n",
        "import pickle\n",
        "import gdown\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"wikipedia\", \"20220301.simple\")\n",
        "\n",
        "data = dataset['train'][:20000]['text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "33aa2b316db44827b700c41921ea0136",
            "230441a10bd747dc8215f7158648f040",
            "de9d6a8ca7d14ac286463a948e76428d",
            "0d37fe12eeb54cd1bb812f39aa415ae0",
            "3fccafa4e3f046fd80f274ceb0e1adc1",
            "6fc7d5c52b144fd18b224078e9926ec3",
            "0e55044647fa48d29c47f95b4c40d573",
            "fad8b76cb7914d49af045cae6ef5cf06",
            "4ceb8870bf294bfc80309bfbbf9a03a0",
            "44abf1c38b4242af9da71b082fd4e69e",
            "968d06b4237444929c0f3c55ba3e61eb"
          ]
        },
        "id": "AoeyVDv9uDwx",
        "outputId": "c4458e9a-725c-46d5-e208-86d304ecede4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset wikipedia (/root/.cache/huggingface/datasets/wikipedia/20220301.simple/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "33aa2b316db44827b700c41921ea0136"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#run this cell only the first time to create and save the tokenizer and the date\n",
        "dump = True\n",
        "\n",
        "tokenizer = Tokenizer(split=' ', filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n', num_words=10000, oov_token='<unk>')\n",
        "\n",
        "corpus = []\n",
        "\n",
        "# Split of each piece of text into sentences\n",
        "for elem in data:\n",
        "  corpus += elem.lower().replace(\"\\n\", \"\").split(\".\")[:]\n",
        "\n",
        "print(\"corpus dim: \",len(corpus))\n",
        "\n",
        "#add a start and an end token\n",
        "corpus = ['<start> '+s+' <end>' for s in corpus]\n",
        "\n",
        "\n",
        "# Tokenization\t\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "#print(tokenizer.word_index['<unk>'])\n",
        "\n",
        "if dump:\n",
        "    with open('tokenizer.pickle', 'wb') as handle:\n",
        "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "original_data = [sen for sen in tokenizer.texts_to_sequences(corpus) if (len(sen) <= 32 and len(sen)>4 and not(1 in sen))]\n",
        "\n",
        "if dump:\n",
        "    with open('original.pickle', 'wb') as handle:\n",
        "        pickle.dump(original_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "print (\"filtered sentences: \",len(original_data))\n",
        "\n",
        "sos = tokenizer.word_index['<start>']\n",
        "eos = tokenizer.word_index['<end>']\n",
        "#print(eos)\n",
        "#print(tokenizer.index_word[sos])\n",
        "\n",
        "tokenizer.word_index['<pad>'] = 0\n",
        "tokenizer.index_word[0] = '<pad>'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzcYlWm8trh9",
        "outputId": "536b5810-7f18-487c-cfc0-e63de7db5573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corpus dim:  510023\n",
            "filtered sentences:  137301\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now create two additional datasets. \n",
        "* shuffled_data contains scrumbled sequences, and will be the input to the model. \n",
        "* target_data is the same as original data but offset by one timestep.\n",
        "It is only useful if you plan to do some language modeling with a teacher forcing technique. You might decide to ignore it.\n"
      ],
      "metadata": {
        "id": "K1woGS7a4Ez4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shuffled_data = [random.sample(s[1:-1],len(s)-2) for s in original_data]\n",
        "shuffled_data = [[sos]+s+[eos] for s in shuffled_data]\n",
        "target_data = [s[1:] for s in original_data]"
      ],
      "metadata": {
        "id": "rs4cerfa4D15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us look at some examples:"
      ],
      "metadata": {
        "id": "mGNwATns6hQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i = np.random.randint(len(original_data))\n",
        "print(\"original sentence: \",original_data[i])\n",
        "print(\"shuffled sentecen: \",shuffled_data[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChbvR6ue6lpj",
        "outputId": "8641be63-3ebd-4976-f911-2d13673e6519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original sentence:  [2, 4, 3437, 600, 3830, 50, 7879, 4929, 39, 282, 3]\n",
            "shuffled sentecen:  [2, 4929, 3437, 39, 50, 600, 4, 7879, 282, 3830, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us look at detokenized data:"
      ],
      "metadata": {
        "id": "er0FoTdc8sLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i = np.random.randint(len(original_data))\n",
        "print(\"original sentence: \",tokenizer.sequences_to_texts([original_data[i]])[0])\n",
        "print(\"shuffled sentence: \",tokenizer.sequences_to_texts([shuffled_data[i]])[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMKM9B1w8yWX",
        "outputId": "6ff533f9-2d5a-41c3-bc98-771ed72b6293"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original sentence:  <start> they may be explorers who like to go to far away places where no one has ever been before <end>\n",
            "shuffled sentence:  <start> away no to before has may they ever been places explorers like far where be go who to one <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You goal is to reconstruct the original sentence out of the shuffled one."
      ],
      "metadata": {
        "id": "Kja87gEg9Rje"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Additional material"
      ],
      "metadata": {
        "id": "s6pe2f8h9gmG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we provide a few additional functions that could be useful to you."
      ],
      "metadata": {
        "id": "EA6su74d9o7v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As usual, you are supposed to divide your data in training and test set. Reserve at least 30% of data for testing.\n",
        "\n",
        "You are likely to need a validation set too."
      ],
      "metadata": {
        "id": "MhD75oyt-AO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, c_train, c_test, y_train, y_test = train_test_split(original_data, shuffled_data, target_data, test_size = 0.3, random_state = 42)\n"
      ],
      "metadata": {
        "id": "dIDuV_Sj9oZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depending from the model you plan to build, you might require padding the input sequence"
      ],
      "metadata": {
        "id": "eOjaBx8d-lEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_sequence_len = max([len(x) for x in original_data])\n",
        "\n",
        "x_train = pad_sequences(x_train, maxlen=max_sequence_len, padding='post')\n",
        "x_test = pad_sequences(x_test, maxlen=max_sequence_len, padding='post')\n",
        "c_train = pad_sequences(c_train, maxlen=max_sequence_len, padding='post')\n",
        "c_test = pad_sequences(c_test, maxlen=max_sequence_len, padding='post')\n",
        "y_train = pad_sequences(y_train, maxlen=max_sequence_len, padding='post')\n",
        "y_test = pad_sequences(y_test, maxlen=max_sequence_len, padding='post')"
      ],
      "metadata": {
        "id": "cbZ1tSFN-kWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"x_train size:\", len(x_train))\n",
        "assert(len(x_train)==len(c_train)==len(y_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PVzEwm8-8Yj",
        "outputId": "853b7255-03e1-43da-8ea6-11a16579fdbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train size: 96110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us finally have a look at the distribution of data w.r.t. their lenght."
      ],
      "metadata": {
        "id": "4jrATEiF_mMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.hist([len(x)-2 for x in original_data],27)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 569
        },
        "id": "KmzOMET9_jxp",
        "outputId": "d320396f-145a-486a-b19d-3484df31f11a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 3897.,  5516.,  6180.,  7633., 10474., 11260., 11167., 10501.,\n",
              "         9768.,  8942.,  7828.,  7010.,  6126.,  5236.,  4551.,  3922.,\n",
              "         3260.,  2695.,  2306.,  1922.,  1611.,  1299.,  1126.,   827.,\n",
              "          773.,   586.,   885.]),\n",
              " array([ 3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13., 14., 15.,\n",
              "        16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26., 27., 28.,\n",
              "        29., 30.]),\n",
              " <BarContainer object of 27 artists>)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a sequence-to-sequence (seq2seq) model with attention mechanism built using the Keras API of TensorFlow.\n",
        "\n",
        "The model is split into two main parts: the encoder and the decoder. Each of these parts is composed of LSTM (Long Short-Term Memory) layers. The encoder reads the input sequences and returns its own internal state (hidden state and cell state). This state is used as the initial state for the decoder, which generates the output sequences.\n",
        "\n",
        "The encoder is composed of three LSTM layers, which help to capture more complex patterns in the input sequences. The final hidden and cell states of the encoder are passed to the decoder.\n",
        "\n",
        "The decoder is a single LSTM layer followed by an Attention layer and a Dense layer. The Attention layer enables the decoder to focus on different parts of the encoder's outputs for every step of the decoder's own outputs. It enhances the performance of the seq2seq model especially when handling long sequences.\n",
        "\n",
        "The model uses RMSprop optimizer with a learning rate of 0.001, and is trained to minimize the 'sparse_categorical_crossentropy' loss, which is suitable for multi-class classification tasks.\n",
        "\n",
        "The embedding layer is used to convert input sequences into dense vectors of fixed size which are more suitable for a neural network. The number of distinct words in the vocabulary is defined by vocab_size.\n",
        "\n",
        "The summary of the model would provide a layer-by-layer description of the architecture, the output shapes and the number of parameters."
      ],
      "metadata": {
        "id": "FE57jMCEQEED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import Attention\n",
        "\n",
        "# Model hyperparameters\n",
        "vocab_size = 10000 + 4\n",
        "max_sequence_len = max([len(x) for x in original_data])\n",
        "embed_dim = 100\n",
        "lstm_units = 256\n",
        "\n",
        "# Define the input layer (encoder)\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "enc_emb =  Embedding(input_dim=vocab_size, output_dim=embed_dim)(encoder_inputs)\n",
        "\n",
        "# LSTM 1 \n",
        "encoder_lstm1 = LSTM(lstm_units,return_sequences=True,return_state=True)\n",
        "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
        "\n",
        "# LSTM 2\n",
        "encoder_lstm2 = LSTM(lstm_units,return_sequences=True,return_state=True)\n",
        "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
        "\n",
        "# LSTM 3\n",
        "encoder_lstm3=LSTM(lstm_units, return_state=True, return_sequences=True)\n",
        "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
        "\n",
        "# Set up the decoder, which will use encoder_states as initial state.\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "dec_emb_layer = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# LSTM using encoder_states as initial state\n",
        "decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
        "\n",
        "# Attention layer\n",
        "attn_layer = Attention(name='attention_layer')\n",
        "attn_out = attn_layer([encoder_outputs, decoder_outputs])\n",
        "\n",
        "# Concat attention output and decoder LSTM output\n",
        "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
        "\n",
        "# Dense layer\n",
        "decoder_dense = TimeDistributed(Dense(vocab_size, activation='softmax'))\n",
        "decoder_outputs = decoder_dense(decoder_concat_input)\n",
        "\n",
        "# Define the model \n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=RMSprop(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEk5G1qVONda",
        "outputId": "2a6deb9a-7bd9-4c07-a185-f111be5dd786"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)        (None, None, 100)    1000400     ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " lstm_4 (LSTM)                  [(None, None, 256),  365568      ['embedding_2[0][0]']            \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " lstm_5 (LSTM)                  [(None, None, 256),  525312      ['lstm_4[0][0]']                 \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " embedding_3 (Embedding)        (None, None, 100)    1000400     ['input_4[0][0]']                \n",
            "                                                                                                  \n",
            " lstm_6 (LSTM)                  [(None, None, 256),  525312      ['lstm_5[0][0]']                 \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " lstm_7 (LSTM)                  [(None, None, 256),  365568      ['embedding_3[0][0]',            \n",
            "                                 (None, 256),                     'lstm_6[0][1]',                 \n",
            "                                 (None, 256)]                     'lstm_6[0][2]']                 \n",
            "                                                                                                  \n",
            " attention_layer (Attention)    (None, None, 256)    0           ['lstm_6[0][0]',                 \n",
            "                                                                  'lstm_7[0][0]']                 \n",
            "                                                                                                  \n",
            " concat_layer (Concatenate)     (None, None, 512)    0           ['lstm_7[0][0]',                 \n",
            "                                                                  'attention_layer[0][0]']        \n",
            "                                                                                                  \n",
            " time_distributed_1 (TimeDistri  (None, None, 10004)  5132052    ['concat_layer[0][0]']           \n",
            " buted)                                                                                           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 8,914,612\n",
            "Trainable params: 8,914,612\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the provided code, I've set up a path for saving my model weights to a file named \"model_weights_best.hdf5\". The intention is to keep a record of the best performing model during the training process.\n",
        "\n",
        "I've then initialized a ModelCheckpoint instance from Keras callbacks, which I've configured to monitor the validation loss (or val_loss). I've chosen to monitor this metric because the goal of the training process is to minimize it. If in any epoch, the model achieves a lower validation loss than the previous best, the weights of this model are saved, replacing the previous file. This is controlled by the parameters save_best_only=True and mode='min'.\n",
        "\n",
        "Finally, I've started the training process of my model by calling the fit method. I've passed the training data, specified the batch size to be 64, and set the number of epochs to 30. I've also reserved 20% of my training data for validation, to monitor the model's performance on unseen data during training. To this fit call, I've passed the ModelCheckpoint instance as a callback, so it's invoked at the end of each epoch.\n"
      ],
      "metadata": {
        "id": "8SbpBGboQ_Gn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "During the final epochs of the model's training process, the model continued to improve. In the 29th epoch, the model achieved a training loss of 0.0277 and an accuracy of 0.9993. Upon evaluation, the validation loss was 0.0994 and the validation accuracy was 0.9864. This was an improvement from the previous best validation loss, and therefore, the model weights at this stage were saved to \"model_weights_best.hdf5\".\n",
        "\n",
        "The model further improved during the 30th and final epoch, achieving a training loss of 0.0251 and an accuracy of 0.9995. The validation loss and accuracy were 0.0950 and 0.9869 respectively, marking an improvement over all previous epochs. Therefore, the weights of the model at this stage were saved, replacing the previously stored weights.\n",
        "\n",
        "In conclusion, the model achieved its best performance in the final epoch, with a training accuracy of 0.9995, validation accuracy of 0.9869, and a minimum validation loss of 0.0950. The weights at this point are stored in \"model_weights_best.hdf5\"."
      ],
      "metadata": {
        "id": "gMMSA9ptRMAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Specify the path to save the model\n",
        "filepath = \"model_weights_best.hdf5\"\n",
        "\n",
        "# Initialize the ModelCheckpoint callback\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "# Model fitting with early stopping\n",
        "history = model.fit([c_train, x_train], y_train,\n",
        "          batch_size=64,\n",
        "          epochs=30,\n",
        "          validation_split=0.2,\n",
        "          callbacks=[checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MmfRt16OmA5",
        "outputId": "0abb93a5-b870-450b-ed71-d4d7691c4c37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "1202/1202 [==============================] - ETA: 0s - loss: 2.5565 - accuracy: 0.6585\n",
            "Epoch 1: val_loss improved from inf to 2.34828, saving model to model_weights_best.hdf5\n",
            "1202/1202 [==============================] - 122s 89ms/step - loss: 2.5565 - accuracy: 0.6585 - val_loss: 2.3483 - val_accuracy: 0.6754\n",
            "Epoch 2/30\n",
            "1202/1202 [==============================] - ETA: 0s - loss: 2.1835 - accuracy: 0.6946\n",
            "Epoch 2: val_loss improved from 2.34828 to 2.02923, saving model to model_weights_best.hdf5\n",
            "1202/1202 [==============================] - 83s 69ms/step - loss: 2.1835 - accuracy: 0.6946 - val_loss: 2.0292 - val_accuracy: 0.7197\n",
            "Epoch 3/30\n",
            "1202/1202 [==============================] - ETA: 0s - loss: 1.7245 - accuracy: 0.7623\n",
            "Epoch 3: val_loss improved from 2.02923 to 1.48045, saving model to model_weights_best.hdf5\n",
            "1202/1202 [==============================] - 82s 68ms/step - loss: 1.7245 - accuracy: 0.7623 - val_loss: 1.4804 - val_accuracy: 0.7953\n",
            "Epoch 4/30\n",
            "1202/1202 [==============================] - ETA: 0s - loss: 1.3158 - accuracy: 0.8132\n",
            "Epoch 4: val_loss improved from 1.48045 to 1.15376, saving model to model_weights_best.hdf5\n",
            "1202/1202 [==============================] - 80s 67ms/step - loss: 1.3158 - accuracy: 0.8132 - val_loss: 1.1538 - val_accuracy: 0.8351\n",
            "Epoch 5/30\n",
            "1202/1202 [==============================] - ETA: 0s - loss: 1.0103 - accuracy: 0.8518\n",
            "Epoch 5: val_loss improved from 1.15376 to 0.89796, saving model to model_weights_best.hdf5\n",
            "1202/1202 [==============================] - 81s 68ms/step - loss: 1.0103 - accuracy: 0.8518 - val_loss: 0.8980 - val_accuracy: 0.8702\n",
            "Epoch 6/30\n",
            "1202/1202 [==============================] - ETA: 0s - loss: 0.7779 - accuracy: 0.8845\n",
            "Epoch 6: val_loss improved from 0.89796 to 0.75039, saving model to model_weights_best.hdf5\n",
            "1202/1202 [==============================] - 82s 68ms/step - loss: 0.7779 - accuracy: 0.8845 - val_loss: 0.7504 - val_accuracy: 0.8910\n",
            "Epoch 7/30\n",
            "1202/1202 [==============================] - ETA: 0s - loss: 0.6152 - accuracy: 0.9089\n",
            "Epoch 7: val_loss improved from 0.75039 to 0.59133, saving model to model_weights_best.hdf5\n",
            "1202/1202 [==============================] - 81s 68ms/step - loss: 0.6152 - accuracy: 0.9089 - val_loss: 0.5913 - val_accuracy: 0.9149\n",
            "Epoch 8/30\n",
            "1201/1202 [============================>.] - ETA: 0s - loss: 0.4924 - accuracy: 0.9276\n",
            "Epoch 8: val_loss improved from 0.59133 to 0.49561, saving model to model_weights_best.hdf5\n",
            "1202/1202 [==============================] - 79s 66ms/step - loss: 0.4924 - accuracy: 0.9276 - val_loss: 0.4956 - val_accuracy: 0.9293\n",
            "Epoch 9/30\n",
            "1202/1202 [==============================] - ETA: 0s - loss: 0.4030 - accuracy: 0.9411\n",
            "Epoch 9: val_loss improved from 0.49561 to 0.42485, saving model to model_weights_best.hdf5\n",
            "1202/1202 [==============================] - 81s 67ms/step - loss: 0.4030 - accuracy: 0.9411 - val_loss: 0.4249 - val_accuracy: 0.9393\n",
            "Epoch 10/30\n",
            "1202/1202 [==============================] - ETA: 0s - loss: 0.3355 - accuracy: 0.9517\n",
            "Epoch 10: val_loss improved from 0.42485 to 0.36999, saving model to model_weights_best.hdf5\n",
            "1202/1202 [==============================] - 81s 68ms/step - loss: 0.3355 - accuracy: 0.9517 - val_loss: 0.3700 - val_accuracy: 0.9474\n",
            "Epoch 11/30\n",
            "1202/1202 [==============================] - ETA: 0s - loss: 0.2830 - accuracy: 0.9600\n",
            "Epoch 11: val_loss improved from 0.36999 to 0.32785, saving model to model_weights_best.hdf5\n",
            "1202/1202 [==============================] - 81s 67ms/step - loss: 0.2830 - accuracy: 0.9600 - val_loss: 0.3278 - val_accuracy: 0.9536\n",
            "Epoch 12/30\n",
            "1202/1202 [==============================] - ETA: 0s - loss: 0.2406 - accuracy: 0.9669\n",
            "Epoch 12: val_loss improved from 0.32785 to 0.29118, saving model to model_weights_best.hdf5\n",
            "1202/1202 [==============================] - 79s 66ms/step - loss: 0.2406 - accuracy: 0.9669 - val_loss: 0.2912 - val_accuracy: 0.9593\n",
            "Epoch 13/30\n",
            "1202/1202 [==============================] - ETA: 0s - loss: 0.2061 - accuracy: 0.9725\n",
            "Epoch 13: val_loss improved from 0.29118 to 0.26288, saving model to model_weights_best.hdf5\n",
            "1202/1202 [==============================] - 80s 66ms/step - loss: 0.2061 - accuracy: 0.9725 - val_loss: 0.2629 - val_accuracy: 0.9633\n",
            "Epoch 14/30\n",
            "1202/1202 [==============================] - ETA: 0s - loss: 0.1772 - accuracy: 0.9773\n",
            "Epoch 14: val_loss improved from 0.26288 to 0.24163, saving model to model_weights_best.hdf5\n",
            "1202/1202 [==============================] - 81s 67ms/step - loss: 0.1772 - accuracy: 0.9773 - val_loss: 0.2416 - val_accuracy: 0.9664\n",
            "Epoch 15/30\n",
            "1201/1202 [============================>.] - ETA: 0s - loss: 0.1531 - accuracy: 0.9814\n",
            "Epoch 15: val_loss improved from 0.24163 to 0.21793, saving model to model_weights_best.hdf5\n",
            "1202/1202 [==============================] - 81s 68ms/step - loss: 0.1531 - accuracy: 0.9813 - val_loss: 0.2179 - val_accuracy: 0.9698\n",
            "Epoch 16/30\n",
            "1201/1202 [============================>.] - ETA: 0s - loss: 0.1329 - accuracy: 0.9846\n",
            "Epoch 16: val_loss improved from 0.21793 to 0.19985, saving model to model_weights_best.hdf5\n",
            "1202/1202 [==============================] - 81s 67ms/step - loss: 0.1329 - accuracy: 0.9846 - val_loss: 0.1998 - val_accuracy: 0.9726\n",
            "Epoch 17/30\n",
            "1202/1202 [==============================] - ETA: 0s - loss: 0.1155 - accuracy: 0.9874\n",
            "Epoch 17: val_loss improved from 0.19985 to 0.18350, saving model to model_weights_best.hdf5\n",
            "1202/1202 [==============================] - 82s 68ms/step - loss: 0.1155 - accuracy: 0.9874 - val_loss: 0.1835 - val_accuracy: 0.9750\n",
            "Epoch 18/30\n",
            "1202/1202 [==============================] - ETA: 0s - loss: 0.1009 - accuracy: 0.9897\n",
            "Epoch 18: val_loss improved from 0.18350 to 0.17031, saving model to model_weights_best.hdf5\n",
            "1202/1202 [==============================] - 81s 68ms/step - loss: 0.1009 - accuracy: 0.9897 - val_loss: 0.1703 - val_accuracy: 0.9768\n",
            "Epoch 19/30\n",
            "1201/1202 [============================>.] - ETA: 0s - loss: 0.0883 - accuracy: 0.9917\n",
            "Epoch 19: val_loss improved from 0.17031 to 0.15987, saving model to model_weights_best.hdf5\n",
            "1202/1202 [==============================] - 79s 65ms/step - loss: 0.0883 - accuracy: 0.9917 - val_loss: 0.1599 - val_accuracy: 0.9783\n",
            "Epoch 20/30\n",
            "1201/1202 [============================>.] - ETA: 0s - loss: 0.0777 - accuracy: 0.9932\n",
            "Epoch 20: val_loss improved from 0.15987 to 0.15093, saving model to model_weights_best.hdf5\n",
            "1202/1202 [==============================] - 79s 66ms/step - loss: 0.0777 - accuracy: 0.9932 - val_loss: 0.1509 - val_accuracy: 0.9793\n",
            "Epoch 21/30\n",
            "1202/1202 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.9946\n",
            "Epoch 21: val_loss improved from 0.15093 to 0.14070, saving model to model_weights_best.hdf5\n",
            "1202/1202 [==============================] - 81s 67ms/step - loss: 0.0687 - accuracy: 0.9946 - val_loss: 0.1407 - val_accuracy: 0.9808\n",
            "Epoch 22/30\n",
            "1202/1202 [==============================] - ETA: 0s - loss: 0.0605 - accuracy: 0.9956\n",
            "Epoch 22: val_loss improved from 0.14070 to 0.13306, saving model to model_weights_best.hdf5\n",
            "1202/1202 [==============================] - 81s 67ms/step - loss: 0.0605 - accuracy: 0.9956 - val_loss: 0.1331 - val_accuracy: 0.9820\n",
            "Epoch 23/30\n",
            "1202/1202 [==============================] - ETA: 0s - loss: 0.0539 - accuracy: 0.9966\n",
            "Epoch 23: val_loss improved from 0.13306 to 0.12638, saving model to model_weights_best.hdf5\n",
            "1202/1202 [==============================] - 81s 67ms/step - loss: 0.0539 - accuracy: 0.9966 - val_loss: 0.1264 - val_accuracy: 0.9829\n",
            "Epoch 24/30\n",
            "1202/1202 [==============================] - ETA: 0s - loss: 0.0477 - accuracy: 0.9973\n",
            "Epoch 24: val_loss improved from 0.12638 to 0.12000, saving model to model_weights_best.hdf5\n",
            "1202/1202 [==============================] - 81s 67ms/step - loss: 0.0477 - accuracy: 0.9973 - val_loss: 0.1200 - val_accuracy: 0.9836\n",
            "Epoch 25/30\n",
            "1202/1202 [==============================] - ETA: 0s - loss: 0.0425 - accuracy: 0.9979\n",
            "Epoch 25: val_loss improved from 0.12000 to 0.11524, saving model to model_weights_best.hdf5\n",
            "1202/1202 [==============================] - 80s 67ms/step - loss: 0.0425 - accuracy: 0.9979 - val_loss: 0.1152 - val_accuracy: 0.9843\n",
            "Epoch 26/30\n",
            "1202/1202 [==============================] - ETA: 0s - loss: 0.0380 - accuracy: 0.9984\n",
            "Epoch 26: val_loss improved from 0.11524 to 0.10998, saving model to model_weights_best.hdf5\n",
            "1202/1202 [==============================] - 81s 67ms/step - loss: 0.0380 - accuracy: 0.9984 - val_loss: 0.1100 - val_accuracy: 0.9851\n",
            "Epoch 27/30\n",
            "1202/1202 [==============================] - ETA: 0s - loss: 0.0341 - accuracy: 0.9987\n",
            "Epoch 27: val_loss improved from 0.10998 to 0.10599, saving model to model_weights_best.hdf5\n",
            "1202/1202 [==============================] - 82s 68ms/step - loss: 0.0341 - accuracy: 0.9987 - val_loss: 0.1060 - val_accuracy: 0.9855\n",
            "Epoch 28/30\n",
            "1202/1202 [==============================] - ETA: 0s - loss: 0.0306 - accuracy: 0.9991\n",
            "Epoch 28: val_loss improved from 0.10599 to 0.10194, saving model to model_weights_best.hdf5\n",
            "1202/1202 [==============================] - 80s 67ms/step - loss: 0.0306 - accuracy: 0.9991 - val_loss: 0.1019 - val_accuracy: 0.9860\n",
            "Epoch 29/30\n",
            "1202/1202 [==============================] - ETA: 0s - loss: 0.0277 - accuracy: 0.9993\n",
            "Epoch 29: val_loss improved from 0.10194 to 0.09941, saving model to model_weights_best.hdf5\n",
            "1202/1202 [==============================] - 80s 67ms/step - loss: 0.0277 - accuracy: 0.9993 - val_loss: 0.0994 - val_accuracy: 0.9864\n",
            "Epoch 30/30\n",
            "1202/1202 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9995\n",
            "Epoch 30: val_loss improved from 0.09941 to 0.09505, saving model to model_weights_best.hdf5\n",
            "1202/1202 [==============================] - 81s 67ms/step - loss: 0.0251 - accuracy: 0.9995 - val_loss: 0.0950 - val_accuracy: 0.9869\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('LSTM_AttentionV2.h5')"
      ],
      "metadata": {
        "id": "V-6dxLdVQ9iZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After training, when the model was evaluated on the test set, it achieved an average score of 0.6451."
      ],
      "metadata": {
        "id": "yW-trc4CRrKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from difflib import SequenceMatcher\n",
        "import numpy as np\n",
        "\n",
        "def score(s, p):\n",
        "  match = SequenceMatcher(None, s, p).find_longest_match()\n",
        "  return match.size / len(s)\n",
        "\n",
        "# Define batch size for prediction\n",
        "batch_size = 500\n",
        "\n",
        "# Placeholder for predicted sentences\n",
        "predicted_sentences = []\n",
        "\n",
        "# Predict in batches\n",
        "for i in range(0, len(x_test), batch_size):\n",
        "    # Create decoder input data for current batch\n",
        "    decoder_input_data_test = np.zeros((len(x_test[i:i+batch_size]), max_sequence_len))\n",
        "    \n",
        "    # Generate predictions for current batch\n",
        "    predictions = model.predict([x_test[i:i+batch_size], decoder_input_data_test])\n",
        "    \n",
        "    # Convert predictions to token sequences\n",
        "    predicted_sequences = np.argmax(predictions, axis=-1)\n",
        "    \n",
        "    # Convert token sequences back to words\n",
        "    predicted_sentences_batch = [tokenizer.sequences_to_texts([pred])[0] for pred in predicted_sequences]\n",
        "    \n",
        "    # Add current batch's predicted sentences to the main list\n",
        "    predicted_sentences.extend(predicted_sentences_batch)\n",
        "\n",
        "# After all predictions have been generated, remove <start> and <end> tokens\n",
        "predicted_sentences = [s.replace('<start> ', '').replace(' <end>', '') for s in predicted_sentences]\n",
        "\n",
        "# Now convert y_test sequences back to words\n",
        "true_sentences = [tokenizer.sequences_to_texts([seq])[0] for seq in y_test]\n",
        "true_sentences = [s.replace('<start> ', '').replace(' <end>', '') for s in true_sentences]\n",
        "\n",
        "# Compute scores for each predicted sentence\n",
        "scores = [score(true, predicted) for true, predicted in zip(true_sentences, predicted_sentences)]\n",
        "\n",
        "# Calculate the average score\n",
        "average_score = np.mean(scores)\n",
        "\n",
        "print(\"The average score on the test set is\", average_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xo0zit3uB1A",
        "outputId": "eada72da-8cc5-47d2-f5dd-181b974fb362"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 2s 14ms/step\n",
            "16/16 [==============================] - 0s 11ms/step\n",
            "16/16 [==============================] - 0s 13ms/step\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "16/16 [==============================] - 0s 9ms/step\n",
            "16/16 [==============================] - 0s 8ms/step\n",
            "16/16 [==============================] - 0s 8ms/step\n",
            "16/16 [==============================] - 0s 9ms/step\n",
            "16/16 [==============================] - 0s 9ms/step\n",
            "16/16 [==============================] - 0s 9ms/step\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "16/16 [==============================] - 0s 9ms/step\n",
            "16/16 [==============================] - 0s 12ms/step\n",
            "16/16 [==============================] - 0s 8ms/step\n",
            "16/16 [==============================] - 0s 9ms/step\n",
            "16/16 [==============================] - 0s 8ms/step\n",
            "16/16 [==============================] - 0s 9ms/step\n",
            "16/16 [==============================] - 0s 9ms/step\n",
            "16/16 [==============================] - 0s 8ms/step\n",
            "16/16 [==============================] - 0s 8ms/step\n",
            "16/16 [==============================] - 0s 12ms/step\n",
            "16/16 [==============================] - 0s 9ms/step\n",
            "16/16 [==============================] - 0s 12ms/step\n",
            "16/16 [==============================] - 0s 9ms/step\n",
            "16/16 [==============================] - 0s 8ms/step\n",
            "16/16 [==============================] - 0s 8ms/step\n",
            "16/16 [==============================] - 0s 11ms/step\n",
            "16/16 [==============================] - 0s 9ms/step\n",
            "16/16 [==============================] - 0s 9ms/step\n",
            "16/16 [==============================] - 0s 8ms/step\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "16/16 [==============================] - 0s 11ms/step\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "16/16 [==============================] - 0s 12ms/step\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "16/16 [==============================] - 0s 9ms/step\n",
            "16/16 [==============================] - 0s 8ms/step\n",
            "16/16 [==============================] - 0s 8ms/step\n",
            "16/16 [==============================] - 0s 9ms/step\n",
            "16/16 [==============================] - 0s 9ms/step\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "16/16 [==============================] - 0s 13ms/step\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "16/16 [==============================] - 0s 8ms/step\n",
            "16/16 [==============================] - 0s 11ms/step\n",
            "16/16 [==============================] - 0s 8ms/step\n",
            "16/16 [==============================] - 0s 9ms/step\n",
            "16/16 [==============================] - 0s 8ms/step\n",
            "16/16 [==============================] - 0s 8ms/step\n",
            "16/16 [==============================] - 0s 9ms/step\n",
            "16/16 [==============================] - 0s 9ms/step\n",
            "16/16 [==============================] - 0s 13ms/step\n",
            "16/16 [==============================] - 0s 13ms/step\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "16/16 [==============================] - 0s 8ms/step\n",
            "16/16 [==============================] - 0s 8ms/step\n",
            "16/16 [==============================] - 0s 8ms/step\n",
            "16/16 [==============================] - 0s 9ms/step\n",
            "16/16 [==============================] - 0s 8ms/step\n",
            "16/16 [==============================] - 0s 8ms/step\n",
            "16/16 [==============================] - 0s 8ms/step\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "16/16 [==============================] - 0s 9ms/step\n",
            "16/16 [==============================] - 0s 8ms/step\n",
            "16/16 [==============================] - 0s 8ms/step\n",
            "16/16 [==============================] - 0s 11ms/step\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "16/16 [==============================] - 0s 9ms/step\n",
            "16/16 [==============================] - 0s 8ms/step\n",
            "16/16 [==============================] - 0s 8ms/step\n",
            "16/16 [==============================] - 0s 8ms/step\n",
            "16/16 [==============================] - 0s 11ms/step\n",
            "16/16 [==============================] - 0s 9ms/step\n",
            "16/16 [==============================] - 0s 8ms/step\n",
            "16/16 [==============================] - 0s 8ms/step\n",
            "16/16 [==============================] - 0s 13ms/step\n",
            "16/16 [==============================] - 0s 13ms/step\n",
            "16/16 [==============================] - 0s 9ms/step\n",
            "16/16 [==============================] - 0s 8ms/step\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "16/16 [==============================] - 0s 9ms/step\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "6/6 [==============================] - 0s 10ms/step\n",
            "The average score on the test set is 0.6450932857471109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics"
      ],
      "metadata": {
        "id": "Fo8MazCGBTv3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let s be the source string and p your prediction. The quality of the results will be measured according to the following metric:\n",
        "\n",
        "1.  look for the longest substring w between s and p\n",
        "2.  compute |w|/|s|\n",
        "\n",
        "If the match is exact, the score is 1. \n",
        "\n",
        "When computing the score, you should NON consider the start and end tokens.\n",
        "\n"
      ],
      "metadata": {
        "id": "G0NOkuO0CfPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The longest common substring can be computed with the SequenceMatcher function of difflib, that allows a simple definition of our metric."
      ],
      "metadata": {
        "id": "a-aUrdlXDdVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from difflib import SequenceMatcher\n",
        "\n",
        "def score(s,p):\n",
        "  match = SequenceMatcher(None, s, p).find_longest_match()\n",
        "  #print(match.size)\n",
        "  return (match.size/len(p))"
      ],
      "metadata": {
        "id": "ulpTRdrF_huh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's do an example."
      ],
      "metadata": {
        "id": "RB2YfjXNExM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original = \"at first henry wanted to be friends with the king of france\"\n",
        "generated = \"henry wanted to be friends with king of france at the first\"\n",
        "\n",
        "print(\"your score is \",score(original,generated))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h17C8bVjEwur",
        "outputId": "949a889f-3204-4068-a89f-5d5f88402e49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "your score is  0.5423728813559322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The score must be computed as an average of at least 3K random examples taken form the test set."
      ],
      "metadata": {
        "id": "BET8GqBvFugR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What to deliver"
      ],
      "metadata": {
        "id": "4fwo7xj4GBW1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You are supposed to deliver a single notebook, suitably commented. \n",
        "The notebook should describe a single model, although you may briefly discuss additional attempts you did.\n",
        "\n",
        "The notebook should contain a full trace of the training. \n",
        "Weights should be made available on request.\n",
        "\n",
        "You must also give a clear assesment of the performance of the model, computed with the metric that has been given to you.\n",
        "\n",
        "# Good work!"
      ],
      "metadata": {
        "id": "i6uITuxOGHfJ"
      }
    }
  ]
}